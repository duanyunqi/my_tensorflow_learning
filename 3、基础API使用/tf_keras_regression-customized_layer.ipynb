{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义层次(子类，lambda方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=7, micro=0, releaselevel='final', serial=0)\n",
      "matplotlib 2.2.3\n",
      "numpy 1.16.4\n",
      "pandas 0.23.4\n",
      "sklearn 0.19.2\n",
      "tensorflow 2.0.0-beta1\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.keras搭建回归模型，数据集使用加利福尼亚的房价预测\n",
    "#导入必要的库即版本\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "#import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd,sklearn,tf,keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=(None, 5))\n",
    "layer(tf.zeros([10, 5]))\n",
    "# 输入10✖5的矩阵，输出10✖100的矩阵\n",
    "# 像调用函数一样使用layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[-0.06181267,  0.09704147, -0.13761345, -0.03465421,  0.18842317,\n",
       "         -0.1696481 ,  0.15502922, -0.18862008,  0.16987978, -0.13756454,\n",
       "         -0.11463474, -0.12883791, -0.08111526, -0.06541719, -0.11032693,\n",
       "         -0.06535597, -0.12027916,  0.16505401, -0.14103115, -0.00326119,\n",
       "          0.09702046,  0.08360003,  0.1184449 ,  0.10699759,  0.11155771,\n",
       "         -0.16076797, -0.1686013 , -0.19924384,  0.03357227,  0.12216164,\n",
       "          0.08843572, -0.02543771,  0.13377424,  0.08231364,  0.14927422,\n",
       "         -0.1790871 ,  0.10663743, -0.14404391, -0.02296604,  0.18471698,\n",
       "          0.04134409, -0.13006394,  0.13123281,  0.09964208,  0.0309108 ,\n",
       "         -0.14461914, -0.22873336, -0.21702656, -0.159038  , -0.0040689 ,\n",
       "         -0.21303289,  0.09505968,  0.07770212,  0.09156977, -0.15198764,\n",
       "          0.16175   , -0.05638061, -0.02552576,  0.2186781 , -0.18961386,\n",
       "         -0.09835187,  0.0450062 , -0.05323374, -0.05076207, -0.18288979,\n",
       "          0.23831056, -0.14728954,  0.10328062,  0.19201596,  0.21286242,\n",
       "         -0.05567493, -0.08790916, -0.07126921, -0.23265067, -0.0380955 ,\n",
       "          0.0769936 ,  0.10662074, -0.11147158, -0.02569355,  0.21724097,\n",
       "          0.00289883,  0.11419015,  0.23044373,  0.1237299 , -0.09453122,\n",
       "          0.04100014, -0.12847959, -0.14079496,  0.07959236,  0.10862033,\n",
       "         -0.0128669 , -0.17548235,  0.21633478, -0.20747876, -0.16450077,\n",
       "          0.15079854, -0.02026378,  0.10266139, -0.01583584, -0.09296785],\n",
       "        [ 0.11350401,  0.02540414, -0.12846193,  0.1208223 ,  0.13106935,\n",
       "          0.03221704, -0.00612047,  0.02289002, -0.02987079,  0.05975352,\n",
       "         -0.19579093,  0.05101143, -0.03482246,  0.22812082, -0.17094764,\n",
       "         -0.1968164 ,  0.10180677, -0.06629236,  0.15708463,  0.0738274 ,\n",
       "         -0.09452473,  0.1895542 ,  0.22995542, -0.02715628, -0.09651139,\n",
       "         -0.04523166,  0.08104198, -0.1423841 ,  0.20302327, -0.10883988,\n",
       "          0.1998355 ,  0.0144033 , -0.22168072,  0.03061672,  0.06254776,\n",
       "         -0.15307301,  0.16595958, -0.10728471, -0.08817612, -0.1121099 ,\n",
       "          0.04588683, -0.2159882 ,  0.1455683 , -0.17112404,  0.10625814,\n",
       "          0.02911012, -0.01792912, -0.00750746,  0.0527672 , -0.01592217,\n",
       "         -0.19179384, -0.02801694,  0.10014971,  0.17414062,  0.05789308,\n",
       "          0.15452592,  0.05875401, -0.1474872 ,  0.08987708,  0.00328216,\n",
       "         -0.09229015,  0.05195425, -0.11502971,  0.02597858,  0.1029378 ,\n",
       "          0.21826158, -0.05446479,  0.19935726,  0.2348681 , -0.20114753,\n",
       "         -0.08217254, -0.04435259, -0.17959228, -0.12456053,  0.17066492,\n",
       "          0.09742169,  0.19589476,  0.07492752, -0.15469179,  0.09057821,\n",
       "         -0.06080337,  0.23310234,  0.06001796,  0.10535516, -0.08921185,\n",
       "         -0.12718472, -0.02323665, -0.06130959,  0.16902699, -0.16741289,\n",
       "          0.13762735, -0.00728199, -0.04157014, -0.21113935,  0.21689998,\n",
       "         -0.05464678, -0.00446312,  0.20977639,  0.18130098,  0.19961129],\n",
       "        [-0.05896969,  0.2196924 , -0.22648932, -0.19263022,  0.03810076,\n",
       "         -0.04830682, -0.23558272, -0.02573892, -0.02736236,  0.10626654,\n",
       "         -0.12137975, -0.06760623, -0.11125353,  0.09126787, -0.1945111 ,\n",
       "          0.04409458,  0.2035488 ,  0.19583829, -0.02298628,  0.11533366,\n",
       "          0.12204327,  0.12482314, -0.12974927, -0.18556696, -0.10075708,\n",
       "         -0.1380316 ,  0.02869867, -0.1998001 , -0.13536434, -0.22189882,\n",
       "         -0.22599623,  0.12048315,  0.03045391, -0.1614271 ,  0.13009916,\n",
       "         -0.0579702 ,  0.01244293, -0.10308319,  0.19871576, -0.05587691,\n",
       "         -0.13852865,  0.15964909, -0.00511535,  0.01739816,  0.22800635,\n",
       "         -0.15669605,  0.12950091,  0.04621603, -0.08708145, -0.01005429,\n",
       "         -0.22133203,  0.14471494, -0.1985328 ,  0.05935599,  0.22216009,\n",
       "         -0.22125925,  0.17870583,  0.12371714, -0.1533761 , -0.15708315,\n",
       "         -0.22615723,  0.03076001,  0.01811226, -0.04527007,  0.01120754,\n",
       "          0.1765414 ,  0.2225038 , -0.08377461,  0.01954122, -0.02399643,\n",
       "         -0.07322812,  0.04273815,  0.05318706,  0.18843673, -0.23508956,\n",
       "         -0.1103943 ,  0.2094375 , -0.02850194,  0.00138487,  0.07095383,\n",
       "          0.05167739,  0.06071173,  0.14079194,  0.22645809, -0.21349595,\n",
       "          0.06714271,  0.02835913,  0.09953003,  0.2223986 ,  0.06859182,\n",
       "         -0.02088797,  0.08204933, -0.09019241,  0.21984588,  0.0767834 ,\n",
       "         -0.22212367,  0.2308193 , -0.07060456, -0.2126723 , -0.04838222],\n",
       "        [ 0.16731714,  0.15453647, -0.08698297, -0.19971809,  0.11331029,\n",
       "          0.20667727, -0.01450311, -0.17414707,  0.0512204 ,  0.08095403,\n",
       "          0.21188624,  0.01144858,  0.20362945, -0.03698009,  0.17889167,\n",
       "          0.07289894, -0.15376393,  0.21438487,  0.11278544, -0.13276671,\n",
       "          0.08643939,  0.2029383 , -0.07895511, -0.14532122, -0.23242036,\n",
       "         -0.10053276,  0.14526726,  0.06964104,  0.0047299 ,  0.09586813,\n",
       "         -0.20855376,  0.0252649 , -0.14273632, -0.03961585, -0.13803975,\n",
       "         -0.12993354,  0.05386283,  0.13454102, -0.09047163, -0.15469316,\n",
       "          0.10141112, -0.01411994,  0.01487865,  0.16230683, -0.21881403,\n",
       "         -0.18851481, -0.18289042,  0.06915392,  0.16761078,  0.23671459,\n",
       "         -0.21606742,  0.01392578, -0.00117588, -0.11168353, -0.05477175,\n",
       "          0.17675091, -0.18268728,  0.1266871 , -0.11900251,  0.12482043,\n",
       "          0.06271343, -0.11609326, -0.07417518, -0.12745002, -0.07384142,\n",
       "          0.11756419,  0.16005532, -0.13332148,  0.10811277,  0.08464245,\n",
       "          0.05098896, -0.21878439, -0.11476464, -0.02294461, -0.15865484,\n",
       "         -0.21553488,  0.17639698,  0.00866464,  0.04124431, -0.18252064,\n",
       "         -0.11778156,  0.08033212,  0.11160262,  0.04940997,  0.14989276,\n",
       "          0.18274786, -0.10870999, -0.02348371,  0.12084298, -0.04647461,\n",
       "         -0.19746362, -0.21668911,  0.18355756,  0.16554432, -0.12840578,\n",
       "         -0.14607576,  0.06529899, -0.13942856,  0.04470588,  0.06038232],\n",
       "        [-0.12446831, -0.08199307, -0.1929202 ,  0.14526795, -0.15173756,\n",
       "          0.06404917,  0.06928833, -0.14922844, -0.003566  , -0.03955196,\n",
       "         -0.0202374 ,  0.05893897,  0.02435668,  0.1647406 ,  0.06561576,\n",
       "         -0.16621137,  0.13016106, -0.12309649,  0.16570036,  0.05285142,\n",
       "          0.17235129, -0.12504148, -0.18365337, -0.14543197,  0.11426042,\n",
       "         -0.1139334 , -0.11402196,  0.21162255,  0.05892341, -0.02375159,\n",
       "          0.11767562,  0.16422449, -0.10026239,  0.00120968, -0.1692903 ,\n",
       "          0.00178337,  0.12165104, -0.04620868, -0.11323887, -0.00977652,\n",
       "         -0.00163649, -0.03619815, -0.22915226, -0.10816394,  0.20954846,\n",
       "          0.06564631, -0.22889312,  0.10959332,  0.14373271, -0.08780235,\n",
       "         -0.03703874,  0.23756911, -0.09534691, -0.21553232, -0.08958322,\n",
       "          0.20949735, -0.22634508,  0.21928708,  0.05575101,  0.15282725,\n",
       "          0.00769427, -0.10892303, -0.07142566,  0.23288883,  0.12991466,\n",
       "         -0.07537653,  0.0117809 , -0.11527769,  0.11123513, -0.13608609,\n",
       "          0.22439872,  0.22682028,  0.1390859 ,  0.08847351, -0.15767741,\n",
       "         -0.02827744,  0.15172194, -0.10028251, -0.19421268,  0.13927318,\n",
       "          0.08971925,  0.06967659,  0.1190374 , -0.09656668, -0.2301958 ,\n",
       "         -0.20069364, -0.11846484,  0.21514164, -0.09192733,  0.1972531 ,\n",
       "         -0.16159579, -0.03664361,  0.00297737,  0.05202971,  0.11816381,\n",
       "          0.11379187, -0.19180559,  0.20163973, -0.21759535,  0.12565587]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables\n",
    "# 打印出layer中的所有参数\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then\n",
      " |  it is flattened prior to the initial dot product with `kernel`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |          Note, if the current variable scope is marked as non-trainable\n",
      " |          then this parameter is ignored and any added variables are also\n",
      " |          marked as non-trainable. `trainable` defaults to `True` unless\n",
      " |          `synchronization` is set to `ON_READ`.\n",
      " |        constraint: constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
      " |          `collections`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.  Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#layer中的其他方法，使用help查看\n",
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California housing dataset.\n",
      "\n",
      "The original database is available from StatLib\n",
      "\n",
      "    http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The data contains 20,640 observations on 9 variables.\n",
      "\n",
      "This dataset contains the average house value as target variable\n",
      "and the following input variables (features): average income,\n",
      "housing average age, average rooms, average bedrooms, population,\n",
      "average occupation, latitude, and longitude in that order.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "Statistics and Probability Letters, 33 (1997) 291-297.\n",
      "\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
      "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
      "         3.78800000e+01, -1.22230000e+02],\n",
      "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
      "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
      "         3.78600000e+01, -1.22220000e+02],\n",
      "       [ 7.25740000e+00,  5.20000000e+01,  8.28813559e+00,\n",
      "         1.07344633e+00,  4.96000000e+02,  2.80225989e+00,\n",
      "         3.78500000e+01, -1.22240000e+02],\n",
      "       [ 5.64310000e+00,  5.20000000e+01,  5.81735160e+00,\n",
      "         1.07305936e+00,  5.58000000e+02,  2.54794521e+00,\n",
      "         3.78500000e+01, -1.22250000e+02],\n",
      "       [ 3.84620000e+00,  5.20000000e+01,  6.28185328e+00,\n",
      "         1.08108108e+00,  5.65000000e+02,  2.18146718e+00,\n",
      "         3.78500000e+01, -1.22250000e+02]])\n",
      "array([4.526, 3.585, 3.521, 3.413, 3.422])\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(housing.data[:5])\n",
    "pprint.pprint(housing.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "# 拆分训练集、测试集、验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 默认按3：1的比例拆分\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# 训练机上获得均值和方差，测试集和验证集上用相同的均值和方差\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softplus:log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x:tf.nn.softplus(x))\n",
    "print(customized_softplus([-10.,-5.,0.,5.,10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 12:21:44.813749 18356 ag_logging.py:145] Entity <bound method CustomizedDenseLayer.call of <__main__.CustomizedDenseLayer object at 0x00000273ADEC1588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xcd in position 0: invalid continuation byte (tmp2bmicr5t.py, line 6)\n",
      "W0814 12:21:44.867605 18356 ag_logging.py:145] Entity <bound method CustomizedDenseLayer.call of <__main__.CustomizedDenseLayer object at 0x00000273ADEC17F0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xcd in position 0: invalid continuation byte (tmpe1_gves_.py, line 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method CustomizedDenseLayer.call of <__main__.CustomizedDenseLayer object at 0x00000273ADEC1588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xcd in position 0: invalid continuation byte (tmp2bmicr5t.py, line 6)\n",
      "WARNING: Entity <bound method CustomizedDenseLayer.call of <__main__.CustomizedDenseLayer object at 0x00000273ADEC17F0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xcd in position 0: invalid continuation byte (tmpe1_gves_.py, line 6)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_6 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_7 (Cu (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 自定义全连接层/customized dense layer\n",
    "# 继承keras.layers.Layer\n",
    "# 定义一个子类，子类中定义三种方法，三种方法实现不同的功能\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    # 重载方法\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"构建所需要的参数\"\"\"\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[1], self.units),\n",
    "                                     initializer='uniform',\n",
    "                                     trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                   shape=(self.units,),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x):\n",
    "        \"\"\"完成正向计算\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu', input_shape=x_train.shape[1:]),\n",
    "    #最后一层一个神经元\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus,\n",
    "    # keras.layers.Dense(1, activation=\"softplus\"),\n",
    "    # keras.layers.Dense(1), keras.layers.Activation('softplus')\n",
    "])\n",
    "model.summary()\n",
    "# 编译，目标函数使用均方差，优化方法使用随机梯度下降，也可以自己定义\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "# 使用earlystopping\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 1.0509 - val_loss: 0.6277\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.5425 - val_loss: 0.5354\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4828 - val_loss: 0.4929\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4616 - val_loss: 0.4731\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4458 - val_loss: 0.4616\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.4375 - val_loss: 0.4442\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4328 - val_loss: 0.4403\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4213 - val_loss: 0.4281\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4218 - val_loss: 0.4256\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4060 - val_loss: 0.4193\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4006 - val_loss: 0.4143\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4019 - val_loss: 0.4180\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3945 - val_loss: 0.4111\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3901 - val_loss: 0.4014\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3856 - val_loss: 0.3985\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3831 - val_loss: 0.3989\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.3817 - val_loss: 0.3957\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3800 - val_loss: 0.3937\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.3786 - val_loss: 0.3998\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3760 - val_loss: 0.3856\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3759 - val_loss: 0.3838\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3745 - val_loss: 0.3854\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3848 - val_loss: 0.3826\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.3695 - val_loss: 0.3798\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.3732 - val_loss: 0.3825\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3736 - val_loss: 0.3780\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3681 - val_loss: 0.3785\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3658 - val_loss: 0.3737\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3654 - val_loss: 0.3744\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - ETA: 0s - loss: 0.366 - 0s 33us/sample - loss: 0.3650 - val_loss: 0.4072\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3638 - val_loss: 0.3713\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3631 - val_loss: 0.3786\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3696 - val_loss: 0.4198\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3808 - val_loss: 0.3698\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3675 - val_loss: 0.3697\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3621 - val_loss: 0.3664\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3593 - val_loss: 0.3695\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3591 - val_loss: 0.3668\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3579 - val_loss: 0.3639\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3603 - val_loss: 0.3656\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3555 - val_loss: 0.3744\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3551 - val_loss: 0.3675\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: 0.3539 - val_loss: 0.3596\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3528 - val_loss: 0.3602\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3538 - val_loss: 0.3633\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: 0.3508 - val_loss: 0.3599\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3587 - val_loss: 0.3597\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3503 - val_loss: 0.3608\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                   validation_data = (x_valid_scaled, y_valid),\n",
    "                   epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
